{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6 - Cobert - Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vuPAxIWWz0MAv-BinmzZwLGc1xyTcoya",
      "authorship_tag": "ABX9TyOSWVZr3WLHIXhVcYwLbLLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3d760f26cb54f2d8ce5dd77edd5e4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1cfd0aed697047279f38e1353b6b6fea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6a1f750586d04038b6a673356bd4b099",
              "IPY_MODEL_6a6bedf7b7794703af777982f171dd75"
            ]
          }
        },
        "1cfd0aed697047279f38e1353b6b6fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a1f750586d04038b6a673356bd4b099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bb008bd5b484d1889ac03aafa5cb394",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 871891,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 871891,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7993c790788242d1ae487c8f6f643490"
          }
        },
        "6a6bedf7b7794703af777982f171dd75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32f96bba68324ea5b3f1cd2d51f22ed4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872k/872k [00:06&lt;00:00, 140kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2d01befea0b4f7c96f4e5b99cde12e3"
          }
        },
        "6bb008bd5b484d1889ac03aafa5cb394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7993c790788242d1ae487c8f6f643490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32f96bba68324ea5b3f1cd2d51f22ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2d01befea0b4f7c96f4e5b99cde12e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa16ab6c997a447b908f6504e52e9549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f743de86ac0c424386e340ec59aeb7e7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2fb28f64b7f941479263172d53c2565f",
              "IPY_MODEL_4bd0b69fac4f43a0a21334f66bc47e8a"
            ]
          }
        },
        "f743de86ac0c424386e340ec59aeb7e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fb28f64b7f941479263172d53c2565f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f738ad334b3d4dd3bb727b7c1b1b29f6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1715180,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1715180,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebcc0afa5648444c97e72be61951e334"
          }
        },
        "4bd0b69fac4f43a0a21334f66bc47e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bf1be3bf7d2446d8f841087a7a794ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.72M/1.72M [00:01&lt;00:00, 1.20MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_148ae9bac4244079949e5ca78a76962a"
          }
        },
        "f738ad334b3d4dd3bb727b7c1b1b29f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebcc0afa5648444c97e72be61951e334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bf1be3bf7d2446d8f841087a7a794ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "148ae9bac4244079949e5ca78a76962a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8c70b4803424e458b30d4cc7cfcaf15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e6b51f9ae4e4a5c9050c3c71e219c03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3a61836bdbb44a168a9993ac7c29df4e",
              "IPY_MODEL_8738361577744b91a96164d4c25c37ce"
            ]
          }
        },
        "9e6b51f9ae4e4a5c9050c3c71e219c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a61836bdbb44a168a9993ac7c29df4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc97a10156024e60b0ec8a2ab52a6701",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07153e0ff7cb4d02b1428158cb40b18b"
          }
        },
        "8738361577744b91a96164d4c25c37ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76823cf170204bfa857f33487d424626",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:04&lt;00:00, 6.93B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b4931262340482bb3bc97fd9a778180"
          }
        },
        "bc97a10156024e60b0ec8a2ab52a6701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07153e0ff7cb4d02b1428158cb40b18b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76823cf170204bfa857f33487d424626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b4931262340482bb3bc97fd9a778180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9880c0ccea04ec09859fe2da30ead62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed1f1a3cf90a4c90bb9fb40fc227eb4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0035bcdc638f484d93f135a50b024600",
              "IPY_MODEL_8a34c19145d344cda5d8193e312f16e8"
            ]
          }
        },
        "ed1f1a3cf90a4c90bb9fb40fc227eb4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0035bcdc638f484d93f135a50b024600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ade492c558ae4eaea20aacfbb666bd7d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de38f42c668b4c6c85fc0cde244e3e8c"
          }
        },
        "8a34c19145d344cda5d8193e312f16e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1e8745409354cb38e664aca0568d0f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:00&lt;00:00, 1.55kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_002350cddb2746e0be9f9db8be8a64c7"
          }
        },
        "ade492c558ae4eaea20aacfbb666bd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de38f42c668b4c6c85fc0cde244e3e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1e8745409354cb38e664aca0568d0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "002350cddb2746e0be9f9db8be8a64c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a57a438c890f4da1bbb0423046bedec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_10fb03f2ec8e474ca14c3de95d100dcb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b7190c1249a74064b5a3a5c3a11015b8",
              "IPY_MODEL_a6ac82424fa648ca8284f60965eb2729"
            ]
          }
        },
        "10fb03f2ec8e474ca14c3de95d100dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b7190c1249a74064b5a3a5c3a11015b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02ff2935f317407b9628d58918b91504",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 672271273,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 672271273,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60a546668c9242f1936a46bc278f537e"
          }
        },
        "a6ac82424fa648ca8284f60965eb2729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_86b8b5cdf57b48a89515375a420718fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 672M/672M [00:26&lt;00:00, 25.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c6a497acc174c38b09779b55d123c59"
          }
        },
        "02ff2935f317407b9628d58918b91504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60a546668c9242f1936a46bc278f537e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86b8b5cdf57b48a89515375a420718fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c6a497acc174c38b09779b55d123c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/Ranking/blob/main/6_Cobert_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMbpl6ykejLT"
      },
      "source": [
        "%%capture\n",
        "!pip install -q ujson\n",
        "!apt install -q libomp-dev\n",
        "!pip install -q transformers\n",
        "!python -m pip -q install --upgrade faiss faiss-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqQXHnrnesYZ"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import ujson\n",
        "import torch\n",
        "import faiss\n",
        "import queue\n",
        "import random\n",
        "import pickle\n",
        "import threading\n",
        "import traceback\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import ceil\n",
        "from itertools import islice\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from itertools import accumulate\n",
        "from collections import defaultdict, OrderedDict\n",
        "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast\n",
        "\n",
        "# better pandas viz\n",
        "pd.set_option('display.max_columns', 100)  \n",
        "pd.set_option('display.expand_frame_repr', 100)\n",
        "pd.set_option('max_colwidth', 700)\n",
        "pd.set_option('display.max_rows', 5000)\n",
        "  \n",
        "# save/load pickles\n",
        "def pickle_file(path, data=None):\n",
        "    if data is None:\n",
        "        with open(path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    if data is not None:\n",
        "        with open(path, 'wb') as handle:\n",
        "            pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        " \n",
        "# path base\n",
        "path_base = '/content/drive/MyDrive/ColBERT/ColBERT - FAQ Receita Federal/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "e3d760f26cb54f2d8ce5dd77edd5e4a2",
            "1cfd0aed697047279f38e1353b6b6fea",
            "6a1f750586d04038b6a673356bd4b099",
            "6a6bedf7b7794703af777982f171dd75",
            "6bb008bd5b484d1889ac03aafa5cb394",
            "7993c790788242d1ae487c8f6f643490",
            "32f96bba68324ea5b3f1cd2d51f22ed4",
            "f2d01befea0b4f7c96f4e5b99cde12e3",
            "aa16ab6c997a447b908f6504e52e9549",
            "f743de86ac0c424386e340ec59aeb7e7",
            "2fb28f64b7f941479263172d53c2565f",
            "4bd0b69fac4f43a0a21334f66bc47e8a",
            "f738ad334b3d4dd3bb727b7c1b1b29f6",
            "ebcc0afa5648444c97e72be61951e334",
            "2bf1be3bf7d2446d8f841087a7a794ad",
            "148ae9bac4244079949e5ca78a76962a",
            "a8c70b4803424e458b30d4cc7cfcaf15",
            "9e6b51f9ae4e4a5c9050c3c71e219c03",
            "3a61836bdbb44a168a9993ac7c29df4e",
            "8738361577744b91a96164d4c25c37ce",
            "bc97a10156024e60b0ec8a2ab52a6701",
            "07153e0ff7cb4d02b1428158cb40b18b",
            "76823cf170204bfa857f33487d424626",
            "6b4931262340482bb3bc97fd9a778180",
            "c9880c0ccea04ec09859fe2da30ead62",
            "ed1f1a3cf90a4c90bb9fb40fc227eb4d",
            "0035bcdc638f484d93f135a50b024600",
            "8a34c19145d344cda5d8193e312f16e8",
            "ade492c558ae4eaea20aacfbb666bd7d",
            "de38f42c668b4c6c85fc0cde244e3e8c",
            "b1e8745409354cb38e664aca0568d0f4",
            "002350cddb2746e0be9f9db8be8a64c7",
            "a57a438c890f4da1bbb0423046bedec5",
            "10fb03f2ec8e474ca14c3de95d100dcb",
            "b7190c1249a74064b5a3a5c3a11015b8",
            "a6ac82424fa648ca8284f60965eb2729",
            "02ff2935f317407b9628d58918b91504",
            "60a546668c9242f1936a46bc278f537e",
            "86b8b5cdf57b48a89515375a420718fc",
            "7c6a497acc174c38b09779b55d123c59"
          ]
        },
        "id": "4n8XH3VDd6Um",
        "outputId": "a42c270a-773e-48d6-c3d0-51c499b624a8"
      },
      "source": [
        "# =============\n",
        "# âœ¨ Constants\n",
        "# =============\n",
        "bsize = 16 # N\n",
        "query_maxlen = 48\n",
        "doc_maxlen = 128\n",
        "path_model = 'bert-base-multilingual-uncased'\n",
        "\n",
        "# ==================\n",
        "# âœ¨ QueryTokenizer\n",
        "# ==================\n",
        "class QueryTokenizer():\n",
        "    def __init__(self, query_maxlen, path_tokenizer):\n",
        "        self.tok = BertTokenizerFast.from_pretrained(path_tokenizer)\n",
        "        self.query_maxlen = query_maxlen\n",
        "\n",
        "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
        "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
        "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
        "\n",
        "    def tokenize(self, batch_text, add_special_tokens=False):\n",
        "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
        "\n",
        "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
        "\n",
        "        if not add_special_tokens:\n",
        "            return tokens\n",
        "\n",
        "        prefix, suffix = [self.cls_token], [self.sep_token]\n",
        "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, batch_text, add_special_tokens=False):\n",
        "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
        "\n",
        "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
        "\n",
        "        if not add_special_tokens:\n",
        "            return ids\n",
        "\n",
        "        prefix, suffix = [self.cls_token_id], [self.sep_token_id]\n",
        "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def tensorize(self, batch_text, bsize=None):\n",
        "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
        "\n",
        "        obj = self.tok(batch_text, padding='max_length', truncation=True,\n",
        "                       return_tensors='pt', max_length=self.query_maxlen)\n",
        "\n",
        "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
        "\n",
        "        ids[ids == 0] = self.mask_token_id\n",
        "\n",
        "        if bsize:\n",
        "            batches = _split_into_batches(ids, mask, bsize)\n",
        "            return batches\n",
        "\n",
        "        return ids, mask\n",
        "\n",
        "# ================\n",
        "# âœ¨ DocTokenizer\n",
        "# ================\n",
        "class DocTokenizer():\n",
        "    def __init__(self, doc_maxlen, path_tokenizer):\n",
        "        self.tok = BertTokenizerFast.from_pretrained(path_tokenizer)\n",
        "        self.doc_maxlen = doc_maxlen\n",
        "\n",
        "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
        "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
        "\n",
        "    def tokenize(self, batch_text, add_special_tokens=False):\n",
        "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
        "\n",
        "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
        "\n",
        "        if not add_special_tokens:\n",
        "            return tokens\n",
        "\n",
        "        prefix, suffix = [self.cls_token], [self.sep_token]\n",
        "        tokens = [prefix + lst + suffix for lst in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, batch_text, add_special_tokens=False):\n",
        "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
        "\n",
        "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
        "\n",
        "        if not add_special_tokens:\n",
        "            return ids\n",
        "\n",
        "        prefix, suffix = [self.cls_token_id], [self.sep_token_id]\n",
        "        ids = [prefix + lst + suffix for lst in ids]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def tensorize(self, batch_text, bsize=None):\n",
        "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
        "\n",
        "        obj = self.tok(batch_text, padding='longest', truncation='longest_first',\n",
        "                       return_tensors='pt', max_length=self.doc_maxlen)\n",
        "\n",
        "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
        "\n",
        "        if bsize:\n",
        "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
        "            batches = _split_into_batches(ids, mask, bsize)\n",
        "            return batches, reverse_indices\n",
        "\n",
        "        return ids, mask\n",
        "\n",
        "# =====================\n",
        "# âœ¨ tensorize triples\n",
        "# =====================\n",
        "def tensorize_triples(query_tokenizer, doc_tokenizer, queries, positives, negatives, bsize):\n",
        "    assert len(queries) == len(positives) == len(negatives)\n",
        "    assert bsize is None or len(queries) % bsize == 0\n",
        "\n",
        "    N = len(queries)\n",
        "    assert bsize == N\n",
        "    Q_ids, Q_mask = query_tokenizer.tensorize(queries)\n",
        "    D_ids, D_mask = doc_tokenizer.tensorize(positives + negatives)\n",
        "    D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)\n",
        "\n",
        "    # Compute max among {length of i^th positive, length of i^th negative} for i \\in N\n",
        "    maxlens = D_mask.sum(-1).max(0).values\n",
        "\n",
        "    # Sort by maxlens\n",
        "    indices = maxlens.sort().indices\n",
        "    Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]\n",
        "    D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]\n",
        "\n",
        "    (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask\n",
        "\n",
        "    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)\n",
        "    positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)\n",
        "    negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)\n",
        "\n",
        "    batches = []\n",
        "    for (q_ids, q_mask), (p_ids, p_mask), (n_ids, n_mask) in zip(query_batches, positive_batches, negative_batches):\n",
        "        Q = (torch.cat((q_ids, q_ids)), torch.cat((q_mask, q_mask)))\n",
        "        D = (torch.cat((p_ids, n_ids)), torch.cat((p_mask, n_mask)))\n",
        "        batches.append((Q, D))\n",
        "\n",
        "    return batches\n",
        "\n",
        "# =============\n",
        "# âœ¨ Aux funcs\n",
        "# =============\n",
        "def _sort_by_length(ids, mask, bsize):\n",
        "    if ids.size(0) <= bsize:\n",
        "        return ids, mask, torch.arange(ids.size(0))\n",
        "\n",
        "    indices = mask.sum(-1).sort().indices\n",
        "    reverse_indices = indices.sort().indices\n",
        "\n",
        "    return ids[indices], mask[indices], reverse_indices\n",
        "\n",
        "def _split_into_batches(ids, mask, bsize):\n",
        "    batches = []\n",
        "    for offset in range(0, ids.size(0), bsize):\n",
        "        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))\n",
        "\n",
        "    return batches\n",
        "\n",
        "# ===============\n",
        "# âœ¨ LazyBatcher\n",
        "# ===============\n",
        "class LazyBatcher():\n",
        "    def __init__(self, bsize, path, path_tokenizer, query_maxlen, doc_maxlen, mode='train', accumsteps=1):\n",
        "        self.bsize, self.accumsteps = bsize, accumsteps\n",
        "        self.query_tokenizer = QueryTokenizer(query_maxlen=query_maxlen, path_tokenizer=path_tokenizer)\n",
        "        self.doc_tokenizer = DocTokenizer(doc_maxlen=doc_maxlen, path_tokenizer=path_tokenizer)\n",
        "        self.tensorize_triples = partial(tensorize_triples, self.query_tokenizer, self.doc_tokenizer)\n",
        "        self.position = 0\n",
        "        self.mode = mode\n",
        "\n",
        "        self.triples = self._load_triples(path_base)\n",
        "        self.queries = self._load_queries(path_base)\n",
        "        self.collection = self._load_collection(path_base)\n",
        "    \n",
        "    def _load_triples(self, path):\n",
        "        if self.mode == 'train':\n",
        "            path = path+'data/df_FAQ_triplet_IDS_TRAIN.parquet.gzip'\n",
        "        elif self.mode == 'valid':\n",
        "            path = path+'data/df_FAQ_triplet_IDS_VALID.parquet.gzip'\n",
        "\n",
        "        df_triplet = pd.read_parquet(path)\n",
        "        triples = []\n",
        "        for qid, pos_pid, neg_pid in zip(\n",
        "            df_triplet.qid.values,\n",
        "            df_triplet.pos_pid.values,\n",
        "            df_triplet.neg_pid.values\n",
        "            ):\n",
        "            triples.append((qid, pos_pid, neg_pid))\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _load_queries(self, path):\n",
        "        if self.mode == 'train':\n",
        "            qid_to_query_train = path+'data/qid_to_query_TRAIN'\n",
        "            return pickle_file(qid_to_query_train)\n",
        "        elif self.mode == 'valid':\n",
        "            qid_to_query_valid = path+'data/qid_to_query_VALID'\n",
        "            return pickle_file(qid_to_query_valid)\n",
        "\n",
        "    def _load_collection(self, path):\n",
        "        if self.mode == 'train':\n",
        "            pid_to_doc_train = path+'data/pid_to_doc_TRAIN'\n",
        "            return pickle_file(pid_to_doc_train)\n",
        "        elif self.mode == 'valid':\n",
        "            pid_to_doc_valid = path+'data/pid_to_doc_VALID'\n",
        "            return pickle_file(pid_to_doc_valid)\n",
        "        \n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triples)\n",
        "\n",
        "    def __next__(self):\n",
        "        # offsets determines the starting index position of each bag (sequence) in input.\n",
        "        offset, endpos = self.position, min(self.position + self.bsize, len(self.triples))\n",
        "        self.position = endpos\n",
        "\n",
        "        if offset + self.bsize > len(self.triples):\n",
        "            raise StopIteration\n",
        "\n",
        "        queries, positives, negatives = [], [], []\n",
        "\n",
        "        for position in range(offset, endpos):\n",
        "            query, pos, neg = self.triples[position]\n",
        "            query, pos, neg = self.queries[query], self.collection[pos], self.collection[neg]\n",
        "            queries.append(query)\n",
        "            positives.append(pos)\n",
        "            negatives.append(neg)\n",
        "\n",
        "        return self.collate(queries, positives, negatives)\n",
        "\n",
        "    def collate(self, queries, positives, negatives):\n",
        "        assert len(queries) == len(positives) == len(negatives) == self.bsize\n",
        "\n",
        "        return self.tensorize_triples(queries, positives, negatives, self.bsize // self.accumsteps)\n",
        "\n",
        "# ===========\n",
        "# âœ¨ ColBERT\n",
        "# ===========\n",
        "class ColBERT(BertPreTrainedModel):\n",
        "    def __init__(self, config, query_maxlen, doc_maxlen, mask_punctuation, dim=128, similarity_metric='cosine'):\n",
        "\n",
        "        super(ColBERT, self).__init__(config)\n",
        "\n",
        "        self.query_maxlen = query_maxlen\n",
        "        self.doc_maxlen = doc_maxlen\n",
        "        self.similarity_metric = similarity_metric\n",
        "        self.dim = dim\n",
        "\n",
        "        self.mask_punctuation = mask_punctuation\n",
        "        self.skiplist = {}\n",
        "\n",
        "        if self.mask_punctuation:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(path_model)\n",
        "            self.skiplist = {w: True\n",
        "                             for symbol in string.punctuation\n",
        "                             for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]}\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.linear = torch.nn.Linear(config.hidden_size, dim, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, Q, D):\n",
        "        return self.score(self.query(*Q), self.doc(*D))\n",
        "\n",
        "    def query(self, input_ids, attention_mask):\n",
        "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
        "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
        "        Q = self.linear(Q)\n",
        "\n",
        "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
        "\n",
        "    def doc(self, input_ids, attention_mask, keep_dims=True):\n",
        "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
        "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
        "        D = self.linear(D)\n",
        "\n",
        "        mask = torch.tensor(self.mask(input_ids), device=DEVICE).unsqueeze(2).float()\n",
        "        D = D * mask\n",
        "\n",
        "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
        "\n",
        "        if not keep_dims:\n",
        "            D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
        "            D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
        "\n",
        "        return D\n",
        "\n",
        "    def score(self, Q, D):\n",
        "        if self.similarity_metric == 'cosine':\n",
        "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
        "\n",
        "        assert self.similarity_metric == 'l2'\n",
        "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)\n",
        "\n",
        "    def mask(self, input_ids):\n",
        "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
        "        return mask\n",
        "\n",
        "# ==================\n",
        "# âœ¨ ModelInference\n",
        "# ==================\n",
        "class ModelInference():\n",
        "    def __init__(self, colbert, path_model):\n",
        "        assert colbert.training is False\n",
        "\n",
        "        self.colbert = colbert\n",
        "        self.query_tokenizer = QueryTokenizer(colbert.query_maxlen, path_tokenizer=path_model)\n",
        "        self.doc_tokenizer = DocTokenizer(colbert.doc_maxlen, path_tokenizer=path_model)\n",
        "\n",
        "    def query(self, *args, to_cpu=False, **kw_args):\n",
        "        with torch.no_grad():\n",
        "            Q = self.colbert.query(*args, **kw_args)\n",
        "            return Q.cpu() if to_cpu else Q\n",
        "\n",
        "    def doc(self, *args, to_cpu=False, **kw_args):\n",
        "        with torch.no_grad():\n",
        "            D = self.colbert.doc(*args, **kw_args)\n",
        "            return D.cpu() if to_cpu else D\n",
        "\n",
        "    def queryFromText(self, queries, bsize=None, to_cpu=False):\n",
        "        if bsize:\n",
        "            batches = self.query_tokenizer.tensorize(queries, bsize=bsize)\n",
        "            batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]\n",
        "            return torch.cat(batches)\n",
        "\n",
        "        input_ids, attention_mask = self.query_tokenizer.tensorize(queries)\n",
        "        return self.query(input_ids, attention_mask)\n",
        "\n",
        "    def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False):\n",
        "        if bsize:\n",
        "            batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)\n",
        "\n",
        "            batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)\n",
        "                       for input_ids, attention_mask in batches]\n",
        "\n",
        "            if keep_dims:\n",
        "                D = _stack_3D_tensors(batches)\n",
        "                return D[reverse_indices]\n",
        "\n",
        "            D = [d for batch in batches for d in batch]\n",
        "            return [D[idx] for idx in reverse_indices.tolist()]\n",
        "\n",
        "        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)\n",
        "        return self.doc(input_ids, attention_mask, keep_dims=keep_dims)\n",
        "\n",
        "    def score(self, Q, D, mask=None, lengths=None, explain=False):\n",
        "        if lengths is not None:\n",
        "            assert mask is None, \"don't supply both mask and lengths\"\n",
        "\n",
        "            mask = torch.arange(D.size(1), device=DEVICE) + 1\n",
        "            mask = mask.unsqueeze(0) <= lengths.to(DEVICE).unsqueeze(-1)\n",
        "\n",
        "        scores = (D @ Q)\n",
        "        scores = scores if mask is None else scores * mask.unsqueeze(-1)\n",
        "        scores = scores.max(1)\n",
        "\n",
        "        if explain:\n",
        "            assert False, \"TODO\"\n",
        "\n",
        "        return scores.values.sum(-1).cpu()\n",
        "\n",
        "def _stack_3D_tensors(groups):\n",
        "    bsize = sum([x.size(0) for x in groups])\n",
        "    maxlen = max([x.size(1) for x in groups])\n",
        "    hdim = groups[0].size(2)\n",
        "\n",
        "    output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)\n",
        "\n",
        "    offset = 0\n",
        "    for x in groups:\n",
        "        endpos = offset + x.size(0)\n",
        "        output[offset:endpos, :x.size(1)] = x\n",
        "        offset = endpos\n",
        "\n",
        "    return output    \n",
        "\n",
        "# - - - - -\n",
        "dataloader_train = LazyBatcher(\n",
        "    bsize=bsize, \n",
        "    path=path_base, \n",
        "    path_tokenizer=path_model,\n",
        "    query_maxlen=query_maxlen,\n",
        "    doc_maxlen=doc_maxlen,\n",
        "    mode='train'\n",
        "    )\n",
        "\n",
        "print('batches:')\n",
        "for i, batches in enumerate(dataloader_train):\n",
        "    print(f' {i }.', end ='')\n",
        "\n",
        "try:\n",
        "    del colbert\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "print()\n",
        "\n",
        "colbert = ColBERT.from_pretrained(\n",
        "    path_model,\n",
        "    query_maxlen=query_maxlen,\n",
        "    doc_maxlen=doc_maxlen,\n",
        "    dim=128,\n",
        "    similarity_metric='cosine',\n",
        "    mask_punctuation=False).to(DEVICE)        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3d760f26cb54f2d8ce5dd77edd5e4a2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa16ab6c997a447b908f6504e52e9549",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1715180.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8c70b4803424e458b30d4cc7cfcaf15",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "batch:\n",
            " 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9880c0ccea04ec09859fe2da30ead62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a57a438c890f4da1bbb0423046bedec5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=672271273.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing ColBERT: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHQggT0seC8O",
        "outputId": "d4fa4572-c4ff-4ed9-f0b0-f42d05b1fa3b"
      },
      "source": [
        "# âœ¨ load colbert from checkpoint\n",
        "colbert.load_state_dict(torch.load(path_base+'data/EPOCH_3_FAQ'))\n",
        "print('\\nmodel loaded!\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model loaded!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Z1t8LCFmxf"
      },
      "source": [
        "# Ranking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV2qbpe84wio"
      },
      "source": [
        "# =========================\n",
        "# âœ¨ Aux funcs for ranking\n",
        "# =========================\n",
        "def batch(group, bsize, provide_offset=False):\n",
        "    offset = 0\n",
        "    while offset < len(group):\n",
        "        L = group[offset: offset + bsize]\n",
        "        yield ((offset, L) if provide_offset else L)\n",
        "        offset += len(L)\n",
        "    return\n",
        "\n",
        "def torch_percentile(tensor, p):\n",
        "    assert p in range(1, 100+1)\n",
        "    assert tensor.dim() == 1\n",
        "\n",
        "    return tensor.kthvalue(int(p * tensor.size(0) / 100.0)).values.item()\n",
        "\n",
        "def load_index_part(filename, verbose=True):\n",
        "    part = torch.load(filename)\n",
        "\n",
        "    if type(part) == list:  # for backward compatibility\n",
        "        part = torch.cat(part)\n",
        "\n",
        "    return part    \n",
        "\n",
        "def load_doclens(directory, flatten=True):\n",
        "    parts, _, _ = get_parts(directory)\n",
        "\n",
        "    doclens_filenames = [os.path.join(directory, 'doclens.{}.json'.format(filename)) for filename in parts]\n",
        "    all_doclens = [ujson.load(open(filename)) for filename in doclens_filenames]\n",
        "\n",
        "    if flatten:\n",
        "        all_doclens = [x for sub_doclens in all_doclens for x in sub_doclens]\n",
        "\n",
        "    return all_doclens\n",
        "\n",
        "def get_parts(directory):\n",
        "    extension = '.pt'\n",
        "\n",
        "    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)\n",
        "                    if filename.endswith(extension)])\n",
        "\n",
        "    assert list(range(len(parts))) == parts, parts\n",
        "\n",
        "    # Integer-sortedness matters.\n",
        "    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]\n",
        "    samples_paths = [os.path.join(directory, '{}.sample'.format(filename)) for filename in parts]\n",
        "\n",
        "    return parts, parts_paths, samples_paths\n",
        "\n",
        "def flatten(L):\n",
        "    return [x for y in L for x in y]\n",
        "\n",
        "def uniq(l):\n",
        "    return list(set(l))\n",
        "\n",
        "# ===========\n",
        "# âœ¨ Metrics\n",
        "# ===========\n",
        "class Metrics:\n",
        "    def __init__(self, mrr_depths:set, recall_depths:set, success_depths:set, total_queries=None):\n",
        "        self.results = {}\n",
        "        self.mrr_sums = {depth:0.0 for depth in mrr_depths}\n",
        "        self.recall_sums = {depth:0.0 for depth in recall_depths}\n",
        "        self.success_sums = {depth:0.0 for depth in success_depths}\n",
        "        self.total_queries = total_queries\n",
        "\n",
        "    def get_result(self, query_idx, query_key, ranking, gold_positives):\n",
        "        assert query_key not in self.results\n",
        "        assert len(self.results) <= query_idx\n",
        "        assert len(set(gold_positives)) == len(gold_positives)\n",
        "        assert len(set([pid for _, pid, _ in ranking])) == len(ranking)\n",
        "\n",
        "        self.results[query_key] = ranking\n",
        "\n",
        "        positives = [i for i, (_, pid, _) in enumerate(ranking) if pid in gold_positives]\n",
        "\n",
        "        if len(positives) == 0:\n",
        "            return\n",
        "\n",
        "        for depth in self.mrr_sums:\n",
        "            first_positive = positives[0]\n",
        "            self.mrr_sums[depth] += (1.0 / (first_positive+1.0)) if first_positive < depth else 0.0\n",
        "\n",
        "        for depth in self.success_sums:\n",
        "            first_positive = positives[0]\n",
        "            self.success_sums[depth] += 1.0 if first_positive < depth else 0.0\n",
        "\n",
        "        for depth in self.recall_sums:\n",
        "            num_positives_up_to_depth = len([pos for pos in positives if pos < depth])\n",
        "            self.recall_sums[depth] += num_positives_up_to_depth / len(gold_positives)\n",
        "\n",
        "    def print_metrics(self, query_idx):\n",
        "        print('- '*10)\n",
        "        for depth in sorted(self.mrr_sums):\n",
        "            mrr_value =  self.mrr_sums[depth] / (query_idx+1.0)\n",
        "            print(f\"MRR@{str(depth):<2} = {mrr_value:.3}\")\n",
        "        \n",
        "        print('- '*10)\n",
        "        for depth in sorted(self.recall_sums):\n",
        "            recall_value = self.recall_sums[depth] / (query_idx+1.0)\n",
        "            print(f\"Recall@{str(depth):<2} = {recall_value:.3}\")\n",
        "        print('- '*10)\n",
        "        for depth in sorted(self.success_sums):\n",
        "            success_value = self.success_sums[depth] / (query_idx+1.0)\n",
        "            print(f\"Success@{str(depth):<2} = {success_value:.3}\")\n",
        "        print('- '*10)\n",
        "\n",
        "def qid2passages(qid, topK_pids, collection, depth):\n",
        "        if collection is not None:\n",
        "            return [collection[pid] for pid in topK_pids[qid][:depth]]\n",
        "        else:\n",
        "            return topK_docs[qid][:depth]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkUYegDIFpj-"
      },
      "source": [
        "# ===============\n",
        "# âœ¨ IndexRanker\n",
        "# ===============\n",
        "class IndexRanker():\n",
        "    def __init__(self, tensor, doclens):\n",
        "        self.tensor = tensor\n",
        "        self.doclens = doclens\n",
        "\n",
        "        self.maxsim_dtype = torch.float32\n",
        "        self.doclens_pfxsum = [0] + list(accumulate(self.doclens))\n",
        "\n",
        "        self.doclens = torch.tensor(self.doclens)\n",
        "        self.doclens_pfxsum = torch.tensor(self.doclens_pfxsum)\n",
        "\n",
        "        self.dim = self.tensor.size(-1)\n",
        "\n",
        "        self.strides = [torch_percentile(self.doclens, p) for p in [25, 50, 75]]\n",
        "        self.strides.append(self.doclens.max().item())\n",
        "        self.strides = sorted(list(set(self.strides)))\n",
        "\n",
        "        self.views = self._create_views(self.tensor)\n",
        "        self.buffers = self._create_buffers(bsize, self.tensor.dtype, {'cpu', 'cuda:0'})\n",
        "\n",
        "    def _create_views(self, tensor):\n",
        "        views = []\n",
        "\n",
        "        for stride in self.strides:\n",
        "            outdim = tensor.size(0) - stride + 1\n",
        "            view = torch.as_strided(tensor, (outdim, stride, self.dim), (self.dim, self.dim, 1))\n",
        "            views.append(view)\n",
        "\n",
        "        return views\n",
        "\n",
        "    def _create_buffers(self, max_bsize, dtype, devices):\n",
        "        buffers = {}\n",
        "\n",
        "        for device in devices:\n",
        "            buffers[device] = [torch.zeros(max_bsize, stride, self.dim, dtype=dtype,\n",
        "                                           device=device, pin_memory=(device == 'cpu'))\n",
        "                               for stride in self.strides]\n",
        "\n",
        "        return buffers\n",
        "\n",
        "    def rank(self, Q, pids, views=None, shift=0):\n",
        "        assert len(pids) > 0\n",
        "        assert Q.size(0) in [1, len(pids)]\n",
        "\n",
        "        Q = Q.contiguous().to(DEVICE).to(dtype=self.maxsim_dtype)\n",
        "\n",
        "        views = self.views if views is None else views\n",
        "        VIEWS_DEVICE = views[0].device\n",
        "\n",
        "        D_buffers = self.buffers[str(VIEWS_DEVICE)]\n",
        "\n",
        "        raw_pids = pids if type(pids) is list else pids.tolist()\n",
        "        pids = torch.tensor(pids) if type(pids) is list else pids\n",
        "\n",
        "        doclens, offsets = self.doclens[pids], self.doclens_pfxsum[pids]\n",
        "\n",
        "        assignments = (doclens.unsqueeze(1) > torch.tensor(self.strides).unsqueeze(0) + 1e-6).sum(-1)\n",
        "\n",
        "        one_to_n = torch.arange(len(raw_pids))\n",
        "        output_pids, output_scores, output_permutation = [], [], []\n",
        "\n",
        "        for group_idx, stride in enumerate(self.strides):\n",
        "            locator = (assignments == group_idx)\n",
        "\n",
        "            if locator.sum() < 1e-5:\n",
        "                continue\n",
        "\n",
        "            group_pids, group_doclens, group_offsets = pids[locator], doclens[locator], offsets[locator]\n",
        "            group_Q = Q if Q.size(0) == 1 else Q[locator]\n",
        "\n",
        "            group_offsets = group_offsets.to(VIEWS_DEVICE) - shift\n",
        "            group_offsets_uniq, group_offsets_expand = torch.unique_consecutive(group_offsets, return_inverse=True)\n",
        "\n",
        "            D_size = group_offsets_uniq.size(0)\n",
        "            D = torch.index_select(views[group_idx], 0, group_offsets_uniq, out=D_buffers[group_idx][:D_size])\n",
        "            D = D.to(DEVICE)\n",
        "            D = D[group_offsets_expand.to(DEVICE)].to(dtype=self.maxsim_dtype)\n",
        "\n",
        "            mask = torch.arange(stride, device=DEVICE) + 1\n",
        "            mask = mask.unsqueeze(0) <= group_doclens.to(DEVICE).unsqueeze(-1)\n",
        "\n",
        "            scores = (D @ group_Q) * mask.unsqueeze(-1)\n",
        "            scores = scores.max(1).values.sum(-1).cpu()\n",
        "\n",
        "            output_pids.append(group_pids)\n",
        "            output_scores.append(scores)\n",
        "            output_permutation.append(one_to_n[locator])\n",
        "\n",
        "        output_permutation = torch.cat(output_permutation).sort().indices\n",
        "        output_pids = torch.cat(output_pids)[output_permutation].tolist()\n",
        "        output_scores = torch.cat(output_scores)[output_permutation].tolist()\n",
        "\n",
        "        assert len(raw_pids) == len(output_pids)\n",
        "        assert len(raw_pids) == len(output_scores)\n",
        "        assert raw_pids == output_pids\n",
        "\n",
        "        return output_scores\n",
        "\n",
        "# =============\n",
        "# âœ¨ IndexPart\n",
        "# =============\n",
        "class IndexPart():\n",
        "    def __init__(self, directory, dim=128, part_range=None, verbose=True):\n",
        "        first_part, last_part = (0, None) if part_range is None else (part_range.start, part_range.stop)\n",
        "\n",
        "        # Load parts metadata\n",
        "        all_parts, all_parts_paths, _ = get_parts(directory)\n",
        "        self.parts = all_parts[first_part:last_part]\n",
        "        self.parts_paths = all_parts_paths[first_part:last_part]\n",
        "\n",
        "        # Load doclens metadata\n",
        "        all_doclens = load_doclens(directory, flatten=False)\n",
        "\n",
        "        self.doc_offset = sum([len(part_doclens) for part_doclens in all_doclens[:first_part]])\n",
        "        self.doc_endpos = sum([len(part_doclens) for part_doclens in all_doclens[:last_part]])\n",
        "        self.pids_range = range(self.doc_offset, self.doc_endpos)\n",
        "\n",
        "        self.parts_doclens = all_doclens[first_part:last_part]\n",
        "        self.doclens = flatten(self.parts_doclens)\n",
        "        self.num_embeddings = sum(self.doclens)\n",
        "\n",
        "        self.tensor = self._load_parts(dim, verbose)\n",
        "        self.ranker = IndexRanker(self.tensor, self.doclens)\n",
        "\n",
        "    def _load_parts(self, dim, verbose):\n",
        "        tensor = torch.zeros(self.num_embeddings + 512, dim, dtype=torch.float16)\n",
        "\n",
        "        offset = 0\n",
        "        for idx, filename in enumerate(self.parts_paths):\n",
        "            print(\"> Loading\", filename, \"\\n\")\n",
        "\n",
        "            endpos = offset + sum(self.parts_doclens[idx])\n",
        "            part = load_index_part(filename, verbose=verbose)\n",
        "\n",
        "            tensor[offset:endpos] = part\n",
        "            offset = endpos\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def pid_in_range(self, pid):\n",
        "        return pid in self.pids_range\n",
        "\n",
        "    def rank(self, Q, pids):\n",
        "\n",
        "        assert Q.size(0) in [1, len(pids)], (Q.size(0), len(pids))\n",
        "        assert all(pid in self.pids_range for pid in pids), self.pids_range\n",
        "\n",
        "        pids_ = [pid - self.doc_offset for pid in pids]\n",
        "        scores = self.ranker.rank(Q, pids_)\n",
        "\n",
        "        return scores\n",
        "\n",
        "# ======================\n",
        "# âœ¨ FaissIndex ranking\n",
        "# ======================\n",
        "class FaissIndex():\n",
        "    def __init__(self, index_path, faiss_index_path, nprobe=8, part_range=None):\n",
        "        print(\"> Loading the FAISS index from\", faiss_index_path, \"..\")\n",
        "\n",
        "        faiss_part_range = os.path.basename(faiss_index_path).split('.')[-2].split('-')\n",
        "\n",
        "        if len(faiss_part_range) == 2:\n",
        "            faiss_part_range = range(*map(int, faiss_part_range))\n",
        "            assert part_range[0] in faiss_part_range, (part_range, faiss_part_range)\n",
        "            assert part_range[-1] in faiss_part_range, (part_range, faiss_part_range)\n",
        "        else:\n",
        "            faiss_part_range = None\n",
        "\n",
        "        self.part_range = part_range\n",
        "        self.faiss_part_range = faiss_part_range\n",
        "\n",
        "        self.faiss_index = faiss.read_index(faiss_index_path)\n",
        "        self.faiss_index.nprobe = nprobe\n",
        "\n",
        "        print(\"> Building the emb2pid mapping\")\n",
        "        all_doclens = load_doclens(index_path, flatten=False)\n",
        "\n",
        "        pid_offset = 0\n",
        "\n",
        "        all_doclens = flatten(all_doclens)\n",
        "\n",
        "        total_num_embeddings = sum(all_doclens)\n",
        "        self.emb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)\n",
        "\n",
        "        offset_doclens = 0\n",
        "        for pid, dlength in enumerate(all_doclens):\n",
        "            self.emb2pid[offset_doclens: offset_doclens + dlength] = pid_offset + pid\n",
        "            offset_doclens += dlength\n",
        "\n",
        "        print(f\"len(self.emb2pid): {len(self.emb2pid)}\" )\n",
        "\n",
        "        self.parallel_pool = Pool(16)\n",
        "\n",
        "    def retrieve(self, faiss_depth, Q, verbose=False):\n",
        "        embedding_ids = self.queries_to_embedding_ids(faiss_depth, Q, verbose=verbose)\n",
        "        pids = self.embedding_ids_to_pids(embedding_ids, verbose=verbose)\n",
        "\n",
        "        return pids\n",
        "\n",
        "    def queries_to_embedding_ids(self, faiss_depth, Q, verbose=True):\n",
        "        # Flatten into a matrix for the faiss search.\n",
        "        num_queries, embeddings_per_query, dim = Q.size()\n",
        "        Q_faiss = Q.view(num_queries * embeddings_per_query, dim).cpu().contiguous()\n",
        "\n",
        "        embeddings_ids = []\n",
        "        faiss_bsize = embeddings_per_query * 5000\n",
        "        for offset in range(0, Q_faiss.size(0), faiss_bsize):\n",
        "            endpos = min(offset + faiss_bsize, Q_faiss.size(0))\n",
        "\n",
        "            some_Q_faiss = Q_faiss[offset:endpos].float().numpy()\n",
        "            _, some_embedding_ids = self.faiss_index.search(some_Q_faiss, faiss_depth)\n",
        "            embeddings_ids.append(torch.from_numpy(some_embedding_ids))\n",
        "\n",
        "        embedding_ids = torch.cat(embeddings_ids)\n",
        "\n",
        "        # Reshape to (number of queries, non-unique embedding IDs per query)\n",
        "        embedding_ids = embedding_ids.view(num_queries, embeddings_per_query * embedding_ids.size(1))\n",
        "\n",
        "        return embedding_ids\n",
        "\n",
        "    def embedding_ids_to_pids(self, embedding_ids, verbose=True):\n",
        "        # Find unique PIDs per query.\n",
        "        all_pids = self.emb2pid[embedding_ids]\n",
        "\n",
        "        all_pids = all_pids.tolist()\n",
        "\n",
        "        if len(all_pids) > 5000:\n",
        "            all_pids = list(self.parallel_pool.map(uniq, all_pids))\n",
        "        else:\n",
        "            all_pids = list(map(uniq, all_pids))\n",
        "\n",
        "        return all_pids\n",
        "\n",
        "# ==========\n",
        "# âœ¨ Ranker\n",
        "# ==========\n",
        "class Ranker():\n",
        "    def __init__(self, index_path, faiss_index_path, inference, faiss_depth=1024):\n",
        "        self.inference = inference\n",
        "        self.faiss_depth = faiss_depth\n",
        "\n",
        "        if faiss_depth is not None:\n",
        "            self.faiss_index = FaissIndex(index_path, faiss_index_path, nprobe=8, part_range=None)\n",
        "            self.retrieve = partial(self.faiss_index.retrieve, self.faiss_depth)\n",
        "\n",
        "        self.index = IndexPart(index_path, dim=inference.colbert.dim, part_range=None, verbose=True)\n",
        "\n",
        "    def encode(self, queries):\n",
        "        assert type(queries) in [list, tuple], type(queries)\n",
        "\n",
        "        Q = self.inference.queryFromText(queries, bsize=512 if len(queries) > 512 else None)\n",
        "\n",
        "        return Q\n",
        "\n",
        "    def rank(self, Q, pids=None):\n",
        "        pids = self.retrieve(Q, verbose=False)[0] if pids is None else pids\n",
        "\n",
        "        assert type(pids) in [list, tuple], type(pids)\n",
        "        assert Q.size(0) == 1, (len(pids), Q.size())\n",
        "        assert all(type(pid) is int for pid in pids)\n",
        "\n",
        "        scores = []\n",
        "        if len(pids) > 0:\n",
        "            Q = Q.permute(0, 2, 1)\n",
        "            scores = self.index.rank(Q, pids)\n",
        "\n",
        "            scores_sorter = torch.tensor(scores).sort(descending=True)\n",
        "            pids, scores = torch.tensor(pids)[scores_sorter.indices].tolist(), scores_sorter.values.tolist()\n",
        "\n",
        "        return pids, scores\n",
        "\n",
        "# ==================\n",
        "# âœ¨ Faiss retrieve\n",
        "# ==================\n",
        "def retrieve(index_path, faiss_index_path, colbert, metrics, queries, qrels, topK_pids, collection, depth):\n",
        "    inference = ModelInference(colbert, path_model)\n",
        "    ranker = Ranker(index_path, faiss_index_path, inference, faiss_depth=depth)\n",
        "\n",
        "    qids_in_order = list(queries.keys())\n",
        "    \n",
        "    for qbatch in batch(qids_in_order, len(queries), provide_offset=False):\n",
        "        qbatch_text = [queries[qid] for qid in qbatch]\n",
        "\n",
        "        rankings = []\n",
        "        for query_idx, q_text in enumerate(qbatch_text):\n",
        "            torch.cuda.synchronize('cuda:0')\n",
        "            Q = ranker.encode([q_text])\n",
        "            pids, scores = ranker.rank(Q)\n",
        "            torch.cuda.synchronize()\n",
        "            rankings.append(zip(pids, scores))\n",
        "\n",
        "        print('Evaluate faiss retrieval')\n",
        "        for query_idx, (qid, ranking) in enumerate(zip(qbatch, rankings)):\n",
        "            rank_qid = [(score, pid, qid2passages(qid, topK_pids, collection, depth)) \n",
        "                            for pid, score in itertools.islice(ranking, depth)]\n",
        "            if qrels:\n",
        "                metrics.get_result(query_idx, qid, rank_qid, qrels[qid])\n",
        "                if query_idx%25 == 0:\n",
        "                    print(f'\\n[{query_idx}]. Query: {queries[qid]}')\n",
        "                    for i, (score, pid, passage) in enumerate(rank_qid, 1):\n",
        "                        if pid in qrels[qid]:\n",
        "                            print(f\"Found at position: {i} with score {score:.3}\")\n",
        "                            print(passage)\n",
        "                            break\n",
        "                    metrics.print_metrics(query_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgVR_auVOsXw"
      },
      "source": [
        "# ===========================\n",
        "# âœ¨ Run retrieval train set\n",
        "# ===========================\n",
        "topK_pids_train  = pickle_file(path_base+'data/topK_pids_TRAIN')\n",
        "topK_docs_train  = pickle_file(path_base+'data/topK_docs_TRAIN')\n",
        "queries_train    = pickle_file(path_base+'data/queries_TRAIN')\n",
        "qrels_train      = pickle_file(path_base+'data/qrels_TRAIN')\n",
        "collection_train = pickle_file(path_base+'data/collection_TRAIN')\n",
        "\n",
        "print('TRAIN OBJECTS')\n",
        "assert len(queries_train) == len(topK_docs_train) == len(topK_pids_train)\n",
        "print(f'\\tlen(queries_train):    {len(queries_train)}')\n",
        "print(f'\\tlen(topK_docs_train):  {len(topK_docs_train)}')\n",
        "print(f'\\tlen(topK_pids_train):  {len(topK_pids_train)}')\n",
        "print(f'\\tlen(collection_train): {len(collection_train)}')\n",
        "\n",
        "metrics = Metrics(\n",
        "        mrr_depths=    {1, 3, 5, 10, 20}, \n",
        "        recall_depths= {1, 3, 5, 10, 20},\n",
        "        success_depths={1, 3, 5, 10, 20},\n",
        "        )\n",
        "\n",
        "retrieve(\n",
        "    index_path=path_base+'index/train_index',\n",
        "    faiss_index_path=path_base+'index/train_index/ivfpq.50.faiss',\n",
        "    colbert=colbert,\n",
        "    metrics=metrics, \n",
        "    queries=queries_train,\n",
        "    qrels=qrels_train,\n",
        "    topK_pids=topK_pids_train,\n",
        "    collection=collection_train, \n",
        "    depth=50,  \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV8XmxlIly9u"
      },
      "source": [
        "# ===========================\n",
        "# âœ¨ Run retrieval valid set\n",
        "# ===========================\n",
        "topK_pids_valid  = pickle_file(path_base+'data/topK_pids_VALID')\n",
        "topK_docs_valid  = pickle_file(path_base+'data/topK_docs_VALID')\n",
        "queries_valid    = pickle_file(path_base+'data/queries_VALID')\n",
        "qrels_valid      = pickle_file(path_base+'data/qrels_VALID')\n",
        "collection_valid = pickle_file(path_base+'data/collection_VALID')\n",
        "\n",
        "print('\\nVALID OBJECTS')\n",
        "assert len(queries_valid) == len(topK_docs_valid) == len(topK_pids_valid)\n",
        "print(f'\\tlen(queries_valid):    {len(queries_valid)}')\n",
        "print(f'\\tlen(topK_docs_valid):  {len(topK_docs_valid)}')\n",
        "print(f'\\tlen(topK_pids_valid):  {len(topK_pids_valid)}')\n",
        "print(f'\\tlen(collection_valid): {len(collection_valid)}')\n",
        "\n",
        "metrics = Metrics(\n",
        "        mrr_depths=    {1, 3, 5, 10, 20}, \n",
        "        recall_depths= {1, 3, 5, 10, 20},\n",
        "        success_depths={1, 3, 5, 10, 20},\n",
        "        )\n",
        "\n",
        "retrieve(\n",
        "    index_path=path_base+'index/valid_index',\n",
        "    faiss_index_path=path_base+'index/valid_index/ivfpq.50.faiss',\n",
        "    colbert=colbert,\n",
        "    metrics=metrics, \n",
        "    queries=queries_valid,\n",
        "    qrels=qrels_valid,\n",
        "    topK_pids=topK_pids_valid,\n",
        "    collection=collection_valid, \n",
        "    depth=50,  \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}